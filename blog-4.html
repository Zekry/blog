<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
  <meta http-equiv="Content-Type" content="text/html" charset="UTF-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />


  <title>Word2Vec Simple Implementation</title>


  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="referrer" content="origin" />
  <meta name="generator" content="Pelican" />
  <link href="https://zekry.github.io/blog/" rel="canonical" />

  <!-- Feed -->
        <link href="https://zekry.github.io/blog/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Zekry's Blog Full Atom Feed" />
          <link href="https://zekry.github.io/blog/feeds/{slug}.atom.xml" type="application/atom+xml" rel="alternate" title="Zekry's Blog Categories Atom Feed" />

  <link href="https://zekry.github.io/blog/theme/css/style.css" type="text/css" rel="stylesheet" />

  <!-- Code highlight color scheme -->
      <link href="https://zekry.github.io/blog/theme/css/code_blocks/github.css" rel="stylesheet">


  <!-- Custom fonts -->
  <link href='https://fonts.googleapis.com/css?family=Montserrat:400,300' rel='stylesheet' type='text/css' />
  <link href="https://fonts.googleapis.com/css?family=Lato" rel="stylesheet" type="text/css" />

  <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
  <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
  <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
    <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
  <![endif]-->


  <link href="https://zekry.github.io/blog/blog-4.html" rel="canonical" />

    <meta name="description" content="Word embeddings is the concept of representing every word used in a language by a set of real numbers (a vector). They are N-dimensional...">

    <meta name="author" content="Zekry">

    <meta name="tags" content="python">
    <meta name="tags" content="API">




<!-- Open Graph -->
<meta property="og:site_name" content="Zekry's Blog"/>
<meta property="og:title" content="Word2Vec Simple Implementation"/>
<meta property="og:description" content="Word embeddings is the concept of representing every word used in a language by a set of real numbers (a vector). They are N-dimensional..."/>
<meta property="og:locale" content="en_US"/>
<meta property="og:url" content="https://zekry.github.io/blog/blog-4.html"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2019-08-01 13:20:00-04:00"/>
<meta property="article:modified_time" content=""/>
<meta property="article:author" content="https://zekry.github.io/blog/author/zekry.html">
<meta property="article:section" content="misc"/>
<meta property="article:tag" content="python"/>
<meta property="article:tag" content="API"/>
<meta property="og:image" content="https://zekry.github.io/blog/theme/images/post-bg.jpg">

<!-- Twitter Card -->

<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Article",
  "name": "Word2Vec Simple Implementation",
  "headline": "Word2Vec Simple Implementation",
  "datePublished": "2019-08-01 13:20:00-04:00",
  "dateModified": "",
  "author": {
    "@type": "Person",
    "name": "Zekry",
    "url": "https://zekry.github.io/blog/author/zekry.html"
  },
  "image": "https://zekry.github.io/blog/theme/images/post-bg.jpg",
  "url": "https://zekry.github.io/blog/blog-4.html",
  "description": "Word embeddings is the concept of representing every word used in a language by a set of real numbers (a vector). They are N-dimensional..."
}
</script>
</head>
<!-- TODO : Body class -->
<body class="home-template">

<nav id="menu">
  <a class="close-button">Close</a>
  <div class="nav-wrapper">
    <p class="nav-label">Menu</p>
    <ul>


    </ul>
  </div>
</nav>
    <!-- Progressbar -->
    <div class="progress-container">
        <span class="progress-bar"></span>
    </div>

    <!-- Page Header -->
    <!-- Set your background image for this header on the line below. -->
    <header id="post-header" class="has-cover">
      <div class="inner">
        <nav id="navigation">
            <span id="home-button" class="nav-button">
                <a class="home-button" href="https://zekry.github.io/blog/" title="Home"><i class="ic ic-arrow-left"></i> Home</a>
            </span>
          <span id="menu-button" class="nav-button">
            <a class="menu-button"><i class="ic ic-menu"></i> Menu</a>
          </span>
        </nav>
        <h1 class="post-title">Word2Vec Simple Implementation</h1>
        <!-- TODO : Proper class for headline -->
        <span class="post-meta">
                <a href="https://zekry.github.io/blog/author/zekry.html">Zekry</a>
            | <time datetime="Thu 01 August 2019">Thu 01 August 2019</time>
        </span>
        <!-- TODO : Modified check -->
            <div class="post-cover cover" style="background-color: indigo">
      </div>
    </header>

  <section id="wrapper">
    <a class="hidden-close"></a>

    <!-- Post content -->
    <main class="content" role="main">
        <article class="post">
        <div class="inner">
            <section class="post-content">
                <p>Word embeddings is the concept of representing every word used in a language by a set of real numbers (a vector). They are N-dimensional vectors that try to capture word-meaning and context in their values. A set of word vectors for a vocabulary should capture the meaning of the word, the relationship to other words, and the context of different words as they are used naturally.</p>
<p>We will be training our own embeddings using Word2Vec. Word2Vec are two-layer neural networks that are trained to produce linguistic contexts of words in the form of numerical vectors. It takes input as a large corpus of text and assigns each unique word in the corpus a corresponding vector in the vector space it created, typically of several hundred dimensions. Word vectors are positioned in the vector space such that words that share common contexts in the corpus are located close to one another.</p>
<p>NLTK provides some corpora that are ready to use for training the model. You can also get any text you like, in which case some cleaning and preprocessing will be required. Let's start...</p>
<p>First we are going to download the corpus from nltk. The below command will prompt the downloader where we can select some or all of the data available.</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">nltk</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span> <span class="p">()</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">showing</span> <span class="n">info</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">raw</span><span class="p">.</span><span class="n">githubusercontent</span><span class="p">.</span><span class="n">com</span><span class="o">/</span><span class="n">nltk</span><span class="o">/</span><span class="n">nltk_data</span><span class="o">/</span><span class="n">gh</span><span class="o">-</span><span class="n">pages</span><span class="o">/</span><span class="k">index</span><span class="p">.</span><span class="n">xml</span>





<span class="k">True</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">brown</span>
<span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">Word2Vec</span>

<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">simplefilter</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">)</span>
</pre></div>


<p>We are going to use the brown corpus, so we just loaded that and imported our model. The last two lines are for ignoring annoying warnings. Now we will assign our data to a variable and take a quick look at it.</p>
<div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">brown</span><span class="o">.</span><span class="n">sents</span><span class="p">()</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">data</span>
</pre></div>


<div class="highlight"><pre><span></span>[[<span class="s1">&#39;</span><span class="s">The</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">Fulton</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">County</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">Grand</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">Jury</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">said</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">Friday</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">an</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">investigation</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">of</span><span class="s1">&#39;</span>, <span class="s2">&quot;</span><span class="s">Atlanta&#39;s</span><span class="s2">&quot;</span>, <span class="s1">&#39;</span><span class="s">recent</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">primary</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">election</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">produced</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">``</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">no</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">evidence</span><span class="s1">&#39;</span>, <span class="s2">&quot;</span><span class="s">&#39;&#39;</span><span class="s2">&quot;</span>, <span class="s1">&#39;</span><span class="s">that</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">any</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">irregularities</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">took</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">place</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">.</span><span class="s1">&#39;</span>], [<span class="s1">&#39;</span><span class="s">The</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">jury</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">further</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">said</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">in</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">term-end</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">presentments</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">that</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">the</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">City</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">Executive</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">Committee</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">,</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">which</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">had</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">over-all</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">charge</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">of</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">the</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">election</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">,</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">``</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">deserves</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">the</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">praise</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">and</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">thanks</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">of</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">the</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">City</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">of</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">Atlanta</span><span class="s1">&#39;</span>, <span class="s2">&quot;</span><span class="s">&#39;&#39;</span><span class="s2">&quot;</span>, <span class="s1">&#39;</span><span class="s">for</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">the</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">manner</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">in</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">which</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">the</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">election</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">was</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">conducted</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">.</span><span class="s1">&#39;</span>], ...]
</pre></div>


<p>Our data is a list of sentences were each sentnce is a list of words. So it is tokenized and ready to be processed by our model.</p>
<p>It’s time to train our model. To do that we simply need to create a new Word2Vec instance. Word2Vec has a variety of parameters that can be optimized, here are some fundamental ones:</p>
<p>• sentences — The iterable over the tokenised sentences we will train on (the Brown sentences).</p>
<p>• size — Dimensionality of the word vectors.</p>
<p>• window — Maximum distance between the current and predicted word within a sentence. For the window of size n the contexts are defined by capturing n words to the left of the target and n words to its right.</p>
<p>• min_count — Ignores all words with total frequency lower than this.</p>
<p>• negative — If &gt; 0, negative sampling will be used, the int for negative specifies how many “noise words” should be drawn (usually between 5-20). If set to 0, no negative sampling is used.</p>
<p>• iter — How many epochs do we want to train for — how many times we want to pass through our training data.</p>
<p>• workers — Determines how many worker threads will be used to train the model.</p>
<div class="highlight"><pre><span></span><span class="n">w2v</span> <span class="o">=</span> <span class="n">Word2Vec</span><span class="p">(</span><span class="n">sentences</span> <span class="o">=</span> <span class="n">data</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="mi">300</span><span class="p">,</span> <span class="n">window</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="n">min_count</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span> <span class="n">negative</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="nb">iter</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="n">workers</span> <span class="o">=</span> <span class="mi">4</span><span class="p">)</span>
</pre></div>


<p>We can see below the length and some of the values for one of the trained word vectors.</p>
<div class="highlight"><pre><span></span><span class="k">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">w2v</span><span class="p">[</span><span class="s1">&#39;craft&#39;</span><span class="p">]))</span>
<span class="n">w2v</span><span class="p">[</span><span class="s1">&#39;craft&#39;</span><span class="p">][:</span><span class="mi">6</span><span class="p">]</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="mi">300</span>





<span class="nb">array</span><span class="p">([</span><span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">2270001</span> <span class="p">,</span>  <span class="mi">0</span><span class="p">.</span><span class="mi">12815301</span><span class="p">,</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">04394055</span><span class="p">,</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">07448043</span><span class="p">,</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">04975335</span><span class="p">,</span>
        <span class="mi">0</span><span class="p">.</span><span class="mi">17514104</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">)</span>
</pre></div>


<p>We can now check word similarity in a couple different ways. For example get the most similar to a certain word:</p>
<div class="highlight"><pre><span></span><span class="n">w2v</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="s1">&#39;mother&#39;</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="p">[(</span><span class="s1">&#39;father&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">.</span><span class="mi">9181399345397949</span><span class="p">),</span>
 <span class="p">(</span><span class="s1">&#39;husband&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">.</span><span class="mi">8962709903717041</span><span class="p">),</span>
 <span class="p">(</span><span class="s1">&#39;wife&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">.</span><span class="mi">8578481674194336</span><span class="p">),</span>
 <span class="p">(</span><span class="s1">&#39;brother&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">.</span><span class="mi">8548943996429443</span><span class="p">),</span>
 <span class="p">(</span><span class="s1">&#39;friend&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">.</span><span class="mi">8197029829025269</span><span class="p">),</span>
 <span class="p">(</span><span class="s1">&#39;daughter&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">.</span><span class="mi">8035991787910461</span><span class="p">),</span>
 <span class="p">(</span><span class="s1">&#39;son&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">.</span><span class="mi">7996119856834412</span><span class="p">),</span>
 <span class="p">(</span><span class="s1">&#39;parents&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">.</span><span class="mi">7619069814682007</span><span class="p">),</span>
 <span class="p">(</span><span class="s1">&#39;fellow&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">.</span><span class="mi">7598813772201538</span><span class="p">),</span>
 <span class="p">(</span><span class="s1">&#39;sister&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">.</span><span class="mi">7593420743942261</span><span class="p">)]</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">w2v</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="s1">&#39;arm&#39;</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="p">[(</span><span class="s1">&#39;shoulders&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">.</span><span class="mi">8915469646453857</span><span class="p">),</span>
 <span class="p">(</span><span class="s1">&#39;fingers&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">.</span><span class="mi">8854344487190247</span><span class="p">),</span>
 <span class="p">(</span><span class="s1">&#39;knees&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">.</span><span class="mi">882630467414856</span><span class="p">),</span>
 <span class="p">(</span><span class="s1">&#39;head&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">.</span><span class="mi">8768821358680725</span><span class="p">),</span>
 <span class="p">(</span><span class="s1">&#39;mouth&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">.</span><span class="mi">8738419413566589</span><span class="p">),</span>
 <span class="p">(</span><span class="s1">&#39;nose&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">.</span><span class="mi">8645551204681396</span><span class="p">),</span>
 <span class="p">(</span><span class="s1">&#39;shoulder&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">.</span><span class="mi">8610392212867737</span><span class="p">),</span>
 <span class="p">(</span><span class="s1">&#39;hat&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">.</span><span class="mi">853934645652771</span><span class="p">),</span>
 <span class="p">(</span><span class="ss">&quot;father&#39;s&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">.</span><span class="mi">8474262952804565</span><span class="p">),</span>
 <span class="p">(</span><span class="s1">&#39;lips&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">.</span><span class="mi">8473799228668213</span><span class="p">)]</span>
</pre></div>


<p>Or pick out the word that does't match in a group of words:</p>
<div class="highlight"><pre><span></span><span class="n">w2v</span><span class="o">.</span><span class="n">doesnt_match</span><span class="p">(</span><span class="s1">&#39;breakfast cereal dinner lunch&#39;</span><span class="o">.</span><span class="n">split</span><span class="p">())</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="s1">&#39;cereal&#39;</span>
</pre></div>


<p>We can also do it this way:</p>
<div class="highlight"><pre><span></span><span class="n">w2v</span><span class="o">.</span><span class="n">doesnt_match</span><span class="p">([</span><span class="s1">&#39;lion&#39;</span><span class="p">,</span> <span class="s1">&#39;tiger&#39;</span><span class="p">,</span> <span class="s1">&#39;box&#39;</span><span class="p">])</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="s1">&#39;box&#39;</span>
</pre></div>


<p>Now our word embeddings and model are ready and can be further used for needed NLP tasks.</p>
            </section>

            <section class="post-info">
                <div class="post-share">
                    <a class="twitter" href="https://twitter.com/share?text=Word2Vec Simple Implementation&amp;url=https://zekry.github.io/blog/blog-4.html" onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
                    <i class="ic ic-twitter"></i><span class="hidden">Twitter</span>
                    </a>
                    <a class="facebook" href="https://www.facebook.com/sharer/sharer.php?u=https://zekry.github.io/blog/blog-4.html" onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;">
                    <i class="ic ic-facebook"></i><span class="hidden">Facebook</span>
                    </a>
                    <a class="googleplus" href="https://plus.google.com/share?url=https://zekry.github.io/blog/blog-4.html" onclick="window.open(this.href, 'google-plus-share', 'width=490,height=530');return false;">
                    <i class="ic ic-googleplus"></i><span class="hidden">Google+</span>
                    </a>
                    <div class="clear"></div>
                </div>

                <aside class="post-tags">
<a href="https://zekry.github.io/blog/tag/python.html">python</a><a href="https://zekry.github.io/blog/tag/api.html">API</a>                </aside>

                <div class="clear"></div>


                </section>


                <aside class="post-nav">
                    <div class="clear"></div>
                </aside>

            </div>
        </article>
    </main>
      <!-- TODO : Body class -->
    <div id="body-class" style="display: none;" class=""></div>

    <footer id="footer">
      <div class="inner">
        <section class="credits">


          <span class="credits-theme">Theme <a href="https://github.com/arulrajnet/attila" rel="nofollow">Attila</a></span>
          <span class="credits-software">Published with <a href="https://github.com/getpelican/pelican" rel="nofollow">Pelican</a></span>
        </section>
      </div>
    </footer>
  </section>

  <script type="text/javascript" src="https://zekry.github.io/blog/theme/js/script.js"></script>

</body>
</html>